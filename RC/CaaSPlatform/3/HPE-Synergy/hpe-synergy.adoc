:Author: Bryan Gartner
:AuthorEMail: Bryan.Gartner@SUSE.com

:CompanyName: SUSE
:ProductName: CaaS Platform

:IHVPartner: HPE
:IHVNetwork: NotApplicable
:IHVPlatform: Synergy 
:IHVPlatformComposer: Composer
:IHVPlatformComposerTech: OneView
:IHVPlatformImager: ImageStreamer
:IHVPlatformModel: 480 Gen10
:IHVPlatformBMC: iLO

= Reference Configuration : {CompanyName} {ProductName} 3 with {IHVPartner} {IHVPlatform}
{Author}, {CompanyName} < {AuthorEMail} >

== Executive Summary
This reference configuration is intended to help an organization create and deploy an on-premise container-as-a-service platform infrastructure to deploy, orchestrate and manage containerized workloads.

For most enterprise-level businesses, the demand for an on-premise, infrastructure to deploy and orchestrate containers is increasing rapidly, allowing the hosting of both production and other phases of workloads development. As with any software-defined infrastructure, all the classic IT disciplines around networking, computing and storage are involved and need to be effectively planned and integrated. This document provides an implementation overview for a production ready deployment with design considerations, implementation suggestions, and best practices. An intelligent, software-defined infrastructure consisting of {CompanyName}(R) {ProductName} technology and {IHVPartner}(R) {IHVPlatform} system hardware enables you to transform your enterprise. With the inherent composability of the combination, delivering container-as-a-service can help reduce costs while still providing flexibility to keep up with your future demands. Leveraging this tested approach, you will have the confidence to deploy a working solution in an agile manner, and be able to maintain and scale it as needed over time.

== Target Audience
This document is intended for IT decision makers, architects, system administrators and technicians who are implementing a flexible, software-defined, container-as-a-service platform. The reader should be familiar with all of the traditional pillars of IT infrastructures -- networking, computing and storage -- along with their local use cases for sizing, scaling and limitations within their environments.

== Solution Overview
Even on the journey to a full https://landscape.cncf.io/[Cloud Native Landscape], the classic IT pillars are still very much in play. General requirements include the need for a small, purpose-built operating system with a container runtime engine and a container orchestration platform to distribute workloads across a target, clustered instance. With all of these attributes in place, both developer and operation teams can effectively deploy, manage and deliver functionality to their end users in a resilient and agile manner.

NOTE: As a further reference, the National Institute of Standards and Technology's (NIST) https://csrc.nist.gov/publications/detail/sp/800-180/draft[Definition of Microservices, Application Containers and System Virtual Machines] describes the important characteristics of application containers. 

While the footprint of each of the containerized microservices is usually much smaller than similar bare-metal or virtual machine workload deployments, the IT pillars must still be designed to readily be composed and accommodate the sheer scale of such high counts of workloads. Core networking technologies, like high-speed, scalable network switches and connectivity can provide the foundational interconnection among computing and workload resources. Physical resource nodes or even virtual machines running atop a hypervisor can deliver the container-as-a-service functionality. A composable infrastructure, like {IHVPartner} {IHVPlatform} provides an ideal target platform, letting users start small and scale-out over time with inherent consistency.  This platform addresses both network and storage resources with industry-leading x86-based servers provided by the {IHVPartner} {IHVPlatform} servers, offering balanced performance and efficiency. Finally, it is the container-as-a service software, {CompanyName} {ProductName}, that provides the user interface for setup, configuration, maintenance, and long term operation of the core software-defined infrastructure requirements bonding them into a cohesive service offering. Each of these components is detailed in the following sections.

== Solution Components

=== Facility
While beyond the scope of this document, the heating, ventilation, air conditioning (HVAC) requirements of hosting such a infrastructure solution should be carefully considered and planned. To aid in determining the power requirements for system deployment, use the https://h20195.www2.hpe.com/v2/GetPDF.aspx/4AA6-2925ENW.pdf[{IHVPartner} Power Advisor] as either an on-line version or a downloadable application. Using this tool, you can plan the needs for your solution and order the correct Power Distribution Unit (PDU) to account for the local power conditions and connections in the final installation location.

=== Network
Networking and the associated services are the technology components which typically requires the most advance planning. Connectivity, capacity and bandwidth requirements for a container-as-a-service infrastructure have a fair amount of complexity, especially when integrated with an existing IT infrastructure and accounting for the magnitude of microservices being deployed.

With the inherent features and composability aspect of {IHVPartner} {IHVPlatform} much of the configuration complexity for network interconnects can easily be codified into a template for each target resource node. This includes the mapping of which network interfaces to which network fabrics within a collection of managed {IHVPartner} {IHVPlatform} frames. The baseline network bandwidth, both for each resource node and collectively within the frame, provided by {IHVPartner} {IHVPlatform} is quite sufficient for a container-as-a-service deployment. While beyond the scope of this document, the only remaining consideration is for capacity and bandwidth for the interconnecting Top-of-Rack switches. In addition access to each of the resource node's {IHVPartner} {IHVPlatformBMC} can be effectively coordinated and managed through the user interface of {IHVPartner} {IHVPlatform} {IHVPlatformComposer}.

=== Computing

ifeval::["{IHVPlatform}" == "Synergy"]

include::../../../../_includes/IHV/{IHVPartner}-{IHVPlatform}.adoc[]

endif::[]

ifeval::["{IHVPlatform}" == "Synergy"]

include::../../../../_includes/IHV/{IHVPartner}-{IHVPlatform}-Frame.adoc[]

endif::[]

ifeval::["{IHVPlatform}" == "Synergy"]

include::../../../../_includes/IHV/{IHVPartner}-{IHVPlatform}-{IHVPlatformComposer}.adoc[]

endif::[]

ifeval::["{IHVPlatform}" == "Synergy"]

include::../../../../_includes/IHV/{IHVPartner}-{IHVPlatform}-{IHVPlatformImager}.adoc[]

endif::[]

ifeval::["{IHVPlatform}" == "Synergy"]

include::../../../../_includes/IHV/{IHVPartner}-{IHVPlatform}-SY.adoc[]

endif::[]

For this the implementation, {IHVPartner} {IHVPlatform} {IHVPlatformModel} servers were utilized for all node roles. Example configurations are included in the Appendices.

NOTE: Any https://www.suse.com/yessearch/[{CompanyName} YES] certified {IHVPartner} platform, like the {IHVPartner} {IHVPlatform} {IHVPlatformModel}, can be used for the physical nodes of this deployment, as long as the certification refers to the major version of the underlying {CompanyName} operating system required by the {CompanyName} {ProductName} release.

=== Storage
Each of the resource nodes is also expected to have some local, direct-attach storage, which is predominantly used for the node's operating system. For this deployment, a pair of disk drives, configured as a RAID1 volume, via the {IHVPartner} {IHVPlatform} {IHVPlatformComposer} templates for the operating system helps provide fewer points of failure.

This storage attribute is also used for the ephemeral container images running on any given node. For more stateful microservice deployments, additional storage technologies can be integrated to house persistent volumes. Such integrations are detailed in other documents.

=== Software

ifeval::["{ProductName}" == "CaaS Platform"]

include::../../../../_includes/Products/SUSE-CaaS-Platform.adoc[]

endif::[]

== Solution Details

This document focuses on a new {CompanyName} {ProductName} deployment which could be scaled over time. A starting point could be a single hypervisor host with virtual machines setup for each of the needed roles to create a basic, minimal cluster to simple evaluate functionality. Over time more physical nodes could be added to augment the cluster or to replace some of the initial, virtual-machine based roles. To provide a production-ready cluster and to take advantage of the {IHVPartner} {IHVPlatform} platform and it's composable features, the following figure shows the target logical cluster deployment:

[[img-DeployLV]]
.Deployment Logical View
image::Deployment-Logical-View.png[Deployment-Logical-View, 640, 480]

=== Deployment Flow
The remainder of this section is meant as a companion guide to the official network, system and software product deployment documentation, citing specific settings as needed for this reference implementation. Default settings are assumed to be in use unless otherwise cited to accomplish the respective best practices and design decisions herein.

Given the very detailed information contained in the https://www.suse.com/documentation/suse-caasp-3/[{CompanyName} {ProductName} Deployment Guide] Guide, only the following additional, incremental configurations and modifications are described below:

Pre-Installation Checklist::
* As preparation, obtain the following software media and documentation artifacts:
** from the https://download.suse.com/Download?buildid=z7ezhywXXRc~[{CompanyName} {ProductName}] site download the {ProductName} x86_64 install media (DVD1)
*** utilize either trial or purchased subscriptions for the all the resource nodes to ensure access to support and software updates. The bill of materials section in the appendices outlines the type and quantity of subscriptions needed.
** in addition, obtain and preview the https://www.suse.com/documentation/suse-caasp-3/index.html[{CompanyName} {ProductName}] documentation, focusing on the:
*** Installation Quick Start
*** Deployment Guide
* Create an {IHVPartner} {IHVPlatform} {IHVPlatformComposer} template to address:
** minimum CPU/Memory/Disk/Networking requirements of {CompanyName} {ProductName}, as noted in https://www.suse.com/documentation/suse-caasp-3/[{CompanyName} {ProductName} Deployment Guide] to be applied to any available {IHVPartner} {IHVPlatform} resource nodes
** consistent and up-to-date versions for BIOS/uEFI/device firmware to reduce potential troubleshooting issues later 
** the BIOS/uEFI settings are reset to defaults for a known baseline, consistent state or with desired values
** use of either local or shared direct-attach storage elements, in a RAID1 configuration, to be used as the operating system installation target
** network interfaces, including {IHVPartner} {IHVPlatformBMC} configured as desired to match the target network topology and connectivity
* Verify the following considerations for various network service configurations are ready to access:
** ensure that you have access to a valid, reliable external NTP service, as this is a critical requirement for all nodes.
** setup external DNS A records for all nodes. Decide on subnet ranges and configure the switch ports accordingly to match those nodes in use.
** ensure access to software security updates and fixes by registering nodes to the http://scc.suse.com[SUSE Customer Center], or by creating a local https://www.suse.com/documentation/sles-12/singlehtml/book_smt/book_smt.html[Subscription Management Tool] service.

Administration Node Installation::
Install the {CompanyName} {ProductName} Platform Administration Node either as a virtual machine or as the first physical resource node
* Apply the {IHVPartner} {IHVPlatform} {IHVPlatformComposer} template to the target resource node in the {IHVPartner} {IHVPlatform} frame
* If a physical node, configure the following {IHVPartner} {IHVPlatformBMC} virtual media image for use
** {CompanyName} {ProductName} ISO image (bootable)

NOTE: The installation process used, across all nodes, whether virtual or physical, was done from ISO images just for consistency in this document. Other options are available as noted in the https://www.suse.com/documentation/suse-caasp-3/[{CompanyName} {ProductName} Deployment Guide].

* Follow the "Installing the Administration Node" process steps described in the https://www.suse.com/documentation/suse-caasp-3/[{CompanyName} {ProductName} Deployment Guide]
* When the installation is complete and the system reboots, use a client system to access the Velum Dashboard web-interface at the Fully-Qualified Doman Name (FQDN) of the Administration Node. Continue the setup described in the "Configuring the Administration Node" section of the https://www.suse.com/documentation/suse-caasp-3/[{CompanyName} {ProductName} Deployment Guide]. Ensure the following items are addressed:
** on the home page, "Create Admin" account with a valid email address and password, then log in
*** check the "Install Tiler (Helm's server component)" box in "Cluster Services" as this will be used extensively later.
*** ensure the Overlay and Service network settings match the desired values, if the default values are not satisfactory.
*** select the desire container runtime. For this deployment, the Docker open source engine was used.
** on the "Bootstrap your CaaS Platform" page:
*** refer to the location of the 'AutoYast' file, in case you'd like to help automate other resource node installations.

Master and Worker Nodes Installation::
Begin installation of the remaining resource nodes. At least three are required for a minimal cluster.

TIP: To provide more resiliency and failover capabilities, it is recommended to install three Master Nodes plus two Worker Nodes. Additional nodes, for master or worker roles, can be added to the cluster or swapped into the cluster at any point in time.

* As with the install of the Administration Node, apply the respective {IHVPartner} {IHVPlatform} {IHVPlatformComposer} template and use the {IHVPartner} {IHVPlatformBMC} virtual media
* Complete the installation steps as described in the "Installing Master and Worker Nodes" section of the https://www.suse.com/documentation/suse-caasp-3/[{CompanyName} {ProductName} Deployment Guide]

HAProxy Setup::
Either a load-balancer or and HAProxy is a suggested approach to make the cluster, and more specifically some of the core Kubernetes Master Node API functions, accessible to client systems. It does this via a virtual IP address which then sends the call to any active masters. While not required for a single master cluster, setting this up in advance allows later expansion and substitutions to happen.
* For HAProxy, this process can be run on any host or virtual machine with access to the cluster network. The steps to deploy this service are described in https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#sec.ha.lb.haproxy[Configuring Load Balancing with HAProxy].

NOTE: As noted in the https://www.suse.com/documentation/suse-caasp-3/[{CompanyName} {ProductName} Deployment Guide], HAProxy should be configured to provide load balancing for both Kubernetes API server (port 6443) and for DEX (OIDC Connect, port 32000). Also ensure that the "stats" configuration stanza does not conflict with any other services (default of port 80) on the target node.

Bootstrap the Cluster::
When all the cluster nodes are installed and rebooted
* Use the client system again to login and access the Velum Dashboard web-interface at the FQDN of the Administration Node to continue the cluster formation.
* There should be a corresponding list of nodes in the "Pending Nodes" section, so "Accept All Nodes"
** Designate the "Master" and "Worker" to the respective nodes, then "Next"
** Enter the Kubernetes Master LB/HAProxy virtual IP FQDN setting for the "External Kubernetes API FQDN"
** Enter the FQDN of the Administration Node in "External Dashboard FQDN", then "Bootstrap Cluster"
** Once this process completes, you should have a fully functional {CompanyName} {ProductName} cluster to use. You can validate this:
** by logging into the Administration Node and running:

----
root@caasp-admin# kubectl cluster-info
root@caasp-admin# kubectl get nodes
root@caasp-admin# kubectl get pods -n kube-system
----

** and also by logging into a client system:
*** using a web browser, login to the Velum Dashboard web-interface with the admin credentials at the FQDN of the Administration Node
*** download the 'kubeconfig' file, and put a copy in the default location of '\~/.kube/config'
*** ensure the client system has `kubectl` installed, then run the same set of `kubectl` commands from the previous section

NOTE: If using a {CompanyName} Linux Enterprise 12 or newer release host as the client, both the `kubectl` and `helm` commands can be found in https://packagehub.suse.com/[{CompanyName} Package Hub]

* To further validate the functionality of the cluster, a representative set of the upstream Kubernetes conformance test suite can be seen at https://github.com/heptio/sonobuoy[Heptio / Sonobuoy]. This can be easily run via a browser on the client system that has access to the admin 'kubeconfig' file as noted in the "Getting Started" section of this site.

=== Additional Considerations

Review the following information to understand the administration aspects of the cluster by reviewing https://www.suse.com/documentation/suse-caasp-3/[{CompanyName} {ProductName} Administration Guide], including:
* Changing or scaling  the cluster node count by adding and removing resource nodes,
* Updating the cluster node's operating system and core packages,
* Setting up resource metrics gathering and visualization,
* Addressing security, by creating users and managing fine-grained, role-based access controls,
* Creating and utilizing a local container image registry, and
* Accessing log locations and creating collection options

== Conclusion
After understanding and working through the steps described in this document, you should have a working container-as-a-service platform that is scalable through the addition of even more resource nodes, as needed. {CompanyName} {ProductName} provides a complete suite of the necessary software and processes which leverages the composability aspect of {IHVPartner} {IHVPlatform} to create a production-worthy and agile platform for deployment of containerized microservice workloads.


== Resources and additional links

{IHVPartner} {IHVPlatform}:: 
* {IHVPartner} {IHVPlatform} {IHVPlatformModel} System - https://www.hpe.com/us/en/product-catalog/synergy/synergy-compute/pip.hpe-synergy-480-gen10-compute-module.1010025863.html

[cols=",,,,", options="header"]
.Bill of Materials - {IHVPartner} {IHVPlatform}
|===
|*_Role_*|*_Quantity_*|*_Product Number_*|*_Product_Description_*|*_Notes_*
|Frame|1|797740-B22|{IHVPartner} {IHVPlatform} 12000 Frame Configure-to-order Frame with 2x FLM 6x Power Supplies 10x Fans|
||1|804353-B22|{IHVPartner} {IHVPlatform} {IHVPlatformComposer}|add second module for HA configuration
|Administration Node|1|871946-B21|{IHVPartner} {IHVPlatform} 480 Gen10 3104 1P 16GB-R S100i SATA Entry Compute Module|
|Master Node|1|871946-B21|{IHVPartner} {IHVPlatform} 480 Gen10 3104 1P 16GB-R S100i SATA Entry Compute Module|recommend 3 for HA configuration
|Worker Node|1|871946-B21|{IHVPartner} {IHVPlatform} 480 Gen10 3104 1P 16GB-R S100i SATA Entry Compute Module|minimum of 2, but can scale to <100
|===

{ProductName}::
* Product Documentation - https://www.suse.com/documentation/suse-caasp-3/

[cols=",,,,", options="header"]
.Bill of Materials - Software
|===
|*_Role_*|*_Quantity_*|*_Product Number_*|*_Description_*|*_Notes_*
|Software|1|Q9K53AAE|{CompanyName} {ProductName}, 1-2 Sockets 3yr 24x7 | for each node of the deployment
|===
