<<<

include::./varsLab2.adoc[]

== Turn Signal On, Ready to Merge
Understand the basics of how to manually interact with Kubernetes and adjustement to an application deployment over its lifecycle.

Goals / Objectives ( Estimated time 25 - 35 minutes )::
Learn how to manually inspect, launch, scale and update a Kubernetes Deployment object. See https://kubernetes.io/docs/tutorials/kubernetes-basics/[Kubernetes Basics] for more examples/details.
+
WARNING: To reduce confusion, the assumption is that the respective Kubernetes cluster being used does not have any extraneous, non-core microservices running. So please cleanup previous or other lab exercise attempts before proceeding.

Assumptions::
The example commands provided in this exercise still use the full syntax and option names. However, alternative ways with abbreviated syntax options are listed below the main command line entries, if you'd like to try those.

Environment::
The following diagram shows the infrastructure components of the lab environment
 which you will interact with.
+
[[img-Lab2-app-lifecycle]]
.Application Launch Environment
image::Lab2-app-lifecycle.png[Lab2-app-lifecycle, 640, 480]

Process::
Login to your student workstation ( *<user>* : _{hostUID}_ , *<password>* : _{hostPW}_ ) footnote:Lstart[link:{videosdir}/L-start.mp4[Lab-start video]]
+
* From the graphical user interface application dock at the bottom of your screen, launch a Terminal.
* In the terminal window, login to the lab environment's "Access Node" using secure shell (`ssh`) by typing
+
[subs="attributes"]
----
ssh {clusterUID}@{clusterAdmIP}
----
+
using the *<password>* _{clusterPW}_
+
NOTE: All of the following process steps should be completed while logged into this lab environment's "Access Node" as mentioned above!
+
* Exercise - Preview and launch the {AuthorGHURL}/{Lab2-dir}/{Lab2Launch-manifest}[application deployment manifest] 
. From the shell prompt, view manifest for the application that is going to be deployed footnote:L2-E1-1[link:{videosdir}/L2-E1-1.mp4[Lab2-Exercise1-1 video]]
+
[subs="attributes"]
----
cat {Lab2-dir}/{Lab2Launch-manifest}
----
+
 **QUIZ**
 a) What is the name of the deployment?
 b) How many replica sets will be running?
 c) Which container image:version will be used for the pods?
+
. Create the application deployment from the designated manifest footnote:L2-E1-2[link:{videosdir}/L2-E1-2.mp4[Lab2-Exercise1-2 video]]
+
[subs="attributes"]
----
kubectl create --filename={Lab2-dir}/{Lab2Launch-manifest}
----
+
TIP: An abbreviated, general version of this command would be `kubectl create -f <manifestFilename>`
+
. Check your answers to the *QUIZ* above, for your active deployment footnote:L2-E1-3[link:{videosdir}/L2-E1-3.mp4[Lab2-Exercise1-3 video]]
+
[subs="attributes,verbatim"]
----
kubectl get deployments nginx-deployment <1>
kubectl get replicasets <2>
kubectl get pods <3>
----
<1> https://kubernetes.io/docs/concepts/workloads/controllers/deployment/[Kubernetes Deployments]
<2> https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/[Kubernetes ReplicaSet]. For now you can ignore the "nfs-client-provisioner" one.
<3> https://kubernetes.io/docs/concepts/workloads/pods/pod/[Kubernetes Pods]. For now you can ignore the "nfs-client-provisioner" one.
+
TIP: An abbreviated version of the 'replicasets' commands would be `kubectl get rs`
+
At this point, you can beging to mentally map the previous command outputs to a graphical representation, shown below, of the specific deployment.
+
[[img-L2-nginx-deployment]]
.Application Deployment
image::NGINX-Deployment.png[NGNIX-MySQL-Deployment, 640, 480]
+
. Continuing with the previous application deployment, obtain a more detailed view of each of the components footnote:L2-E1-4[link:{videosdir}/L2-E1-4.mp4[Lab2-Exercise1-4 video]]
+
[subs="attributes"]
----
kubectl describe deployments nginx-deployment
----
+
 **QUIZ**
 a) Are all the desired "Replicas" created and running?
 b) What events have happened during the deployment?
+
* Exercise - Show how Kubernetes maintains the specified number of replica set pods to provide the desired resiliency functionality
. Get a view of the currently running pods that are part of the replica set footnote:L2-E2-1[link:{videosdir}/L2-E2-1.mp4[Lab2-Exercise2-1 video]]
+
[subs="attributes"]
----
kubectl get pods
----
+
. Let's target the last "nginx" one on the previous commands output. Cut and past the entry from the "NAME" first column, and substituting that string for *<nginx-deployment-ID>* in the command below footnote:L2-E2-2[link:{videosdir}/L2-E2-2.mp4[Lab2-Exercise2-2 video]]
+
[subs="attributes,verbatim"]
----
kubectl delete pod <nginx-deployment-ID>
----
+
TIP: You can also use a shell command-line completion approach with `kubectl`. As an example, `kubectl delete pod n<TAB><TAB>` will return a list of pods that begin with the letter "n", then you only need to type the next character (if unique) followed by another `<TAB>` to finish the command before hitting `<ENTER>` to execute it.
+
The resulting output should convey that the pod was deleted. Now verify that Kubernetes dealt with that issue, ensuring that two pods are always running for your specified replica set
+
[subs="attributes"]
----
kubectl get pods
kubectl get replicasets
kubectl describe replicasets | more
----
+
As you should see, a new pod *<ID>* has been created for the nginx-deployment, and recorded as an event in the replicaset, to keep the desired count always running (and note that the one of the original pair that you did not delete is still running).
+
* Exercise - Manually scale your application deployment to perhaps respond to an increased amount of usage
. View changes in proposed {AuthorGHURL}/{Lab2-dir}/{Lab2Scale-manifest}[deployment] manifest, compared to the original one footnote:L2-E3-1[link:{videosdir}/L2-E3-1.mp4[Lab2-Exercise3-1 video]]
+
[subs="attributes"]
----
diff {Lab2-dir}/{Lab2Launch-manifest} {Lab2-dir}/{Lab2Scale-manifest}
----
+
 **QUIZ**
 a) Which attribute is being modified?
+
. Check the current state footnote:L2-E3-2[link:{videosdir}/L2-E3-2.mp4[Lab2-Exercise3-2 video]]
+
[subs="attributes"]
----
kubectl get replicasets
kubectl get pods
----
+
. Scale the application deployment's replicasets by applying the updated manifest footnote:L2-E3-3[link:{videosdir}/L2-E3-3.mp4[Lab2-Exercise3-3 video]]
+
[subs="attributes"]
----
kubectl apply --filename={Lab2-dir}/{Lab2Scale-manifest}
----
+
. Confirm the expected results footnote:L2-E3-4[link:{videosdir}/L2-E3-4.mp4[Lab2-Exercise3-4 video]]
+
[subs="attributes"]
----
kubectl get replicasets
kubectl get pods
----
+
 **Bonus QUIZ**
 a) How might you scale down the deployment back to the original configuration?
+
* Exercise - Update your application using a new container image version, as if you need to take advantage of security updates or improved functionality
. View changes in proposed {AuthorGHURL}/{Lab2-dir}/{Lab2Update-manifest}[deployment] manifest, compared to the previous, scaling one footnote:L2-E4-1[link:{videosdir}/L2-E4-1.mp4[Lab2-Exercise4-1 video]]
+
[subs="attributes"]
----
diff {Lab2-dir}/{Lab2Scale-manifest} {Lab2-dir}/{Lab2Update-manifest}
----
+
 **QUIZ**
 a) Which attributes are different from the original deployment?
 b) Given the previous exercise, which single change do you expect to see?
+
. Check the current state footnote:L2-E4-2[link:{videosdir}/L2-E4-2.mp4[Lab2-Exercise4-2 video]]
+
[subs="attributes"]
----
kubectl describe deployments nginx-deployment
----
+
NOTE: In `kubectl get deployments` output, you will notice that the previous scaling events has been recorded and logged.
+
. Update the application container image with a newer version footnote:L2-E4-3[link:{videosdir}/L2-E4-3.mp4[Lab2-Exercise4-3 video]]
+
[subs="attributes"]
----
kubectl apply --filename={Lab2-dir}/{Lab2Update-manifest}
----
+
. Confirm the expected results footnote:L2-E4-4[link:{videosdir}/L2-E4-4.mp4[Lab2-Exercise4-4 video]]
+
[subs="attributes"]
----
kubectl describe deployments nginx-deployment
kubectl describe replicasets | more
kubectl describe pods
----
+
 **Bonus QUIZ**
 a) How could you downgrade the container image version back down to the original configuration?
+
NOTE: One can also accomplish such updates with `kubectl set` or with `kubectl edit`. For those approaches, refer to https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment[Kubernetes-Workloads-Controllers-Deployments] documentation.
+
* *_Optional / Advanced Exercises_* - If you have some spare time, or are curious about the commands used or what was deployed, try: footnote:L2-OA[link:{videosdir}/L2-OA.mp4[Lab2-Optional-Advanced video]]
** While `kubectl` is a comprehensive command-line interface to interact with Kubernetes, a recent terminal-based user interface, https://github.com/derailed/k9s[K9s] has also become available. Its goal is to make it easier to navigate, observe and manage your applications. If you'd like to take this for a test drive, use the following steps, in the terminal, while logged into the "Access Node":
. Install this utility
+
[subs="attributes"]
----
tar -zxvf {Lab2-dir}/{Lab2k9s-package}
----
+
. Then beginning learning about the K9s command and launch it
+
[subs="attributes"]
----
./k9s help
./k9s info
./k9s
----
+
You should see an interface like that shown in the following figure
+
[[img-Lab2-k9s]]
.Kubernets CLI To Manage Your Clusters In Style
image::Lab2-k9s.png[Lab2-k9s, 640, 480]
+
. At this point you can explore using the interface by looking over the https://github.com/derailed/k9s[K9s] project GitHub site to determine which keystroke leads you to the desired output.
+
TIP: If you would like to continue running this for the remainder of this hands-on session, simply open another Terminal and log into the "Access Node" to complete those exercises.
+
////
** Another option to interface with Kubernetes is permitted with a web-based interface called the https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/[Dashboard]. This can also be easily installed on the target Kubernetes cluster and utilized for interactions. If you'd like to take this for a test drive, use the following steps, starting in the terminal, while logged into the "Access Node":
. Install this user interface
+
[subs="attributes"]
----
helm install --name my-k8sdash stable/kubernetes-dashboard
----
+
. As noted in the output of the previous command, you need to query the running pod and create a port-forward to access this appliction
+
[subs="attributes"]
----
export POD_NAME=$(kubectl get pods -n default -l "app=kubernetes-dashboard,release=my-k8sdash" -o jsonpath="{.items[0].metadata.name}")
kubectl -n default port-forward $POD_NAME 8443:8443
Ctrl-z
bg
w3m https://127.0.0.1:8443/
----
+
NOTE: FixMe - Testing from Student Workstation browser to K8s Dash = WIP
+
[[img-Lab2-k8sdash]]
.Kubernetes WebUI Dashboard
image::Lab2-k8sdash.png[Lab2-k8sdash, 640, 480]
+
TIP: FixMe - If you would like to continue running this for the remainder of this hands-on session, simply blah, blah. And if not perform the following command to remove this installation `helm delete --purge my-k8sdash`
+
////
* Cleanup
+
IMPORTANT: Ensure that you clean up all your application deployments to return to a known, clean state. Delete the running deployment and check that all the other objects are gone as well.
+
[subs="attributes"]
----
kubectl delete --filename={Lab2-dir}/{Lab2Update-manifest}
kubectl get deployments
kubectl get replicasets
kubectl get pods
----

Knowledge Check::
Initial introduction to the Kubernetes Deployment object
+
* launching and exploring the deployment components
* scaling and updating selected deployment components
* deleting the deployment

