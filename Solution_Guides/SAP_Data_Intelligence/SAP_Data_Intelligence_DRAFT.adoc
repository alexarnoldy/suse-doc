:useCase: Data Management and Machine Learning

:title: Digital Transformation - {ISVPartner} {ISVProductName} 3.0

:author: Alex Arnoldy
:authorEmail: alex.arnoldy@suse.com

# :authorGHURL: https://github.com/alexarnoldy/suse-doc/Solution_Guides

:imagesdir: ../media/

ifdef::env-github[]
:imagesdir: {authorGHURL}/blob/master/SA-{useCase}/media/
endif::[]

:CompanyName: SUSE
:ProductName: NA
:ProductNameNoSpaces: NA
:ProductNameK8sManager: Rancher
:ProductNameK8s: Rancher Kubernetes Engine
:ProductNameK8sShort: RKE
:ProductNamePaaS: Cloud Application Platform
:ProductNameSES: Enterprise Storage
:ProductNameSESshort: SES

:SUSEDocType: Solution Guide
:SUSEDocTypeNoSpaces: Solution-Guide

:MarketCategory: Data Management
:MarketCategoryAbbreviation: Data-Management
:SecondaryMarketCategory: Artifical Intelligence / Machine Learning
:SecondaryMarketCategoryAbbreviation: AI/ML

:IHVPartner: Dell
:IHVProductName: PowerEdge
:IHVProductNameNoSpaces: Data Center Servers

:ISVPartner: SAP
:ISVProductName: Data Intelligence
:ISVProductNameNoSpaces: Data-Intelligence

= {title}
{author}, {companyName} < {authorEMail} >

:favicon:
:doctype: book

[preface]
== Preface

{ISVPartner} {ISVProductName} provides an intuitive, single-pane-of-glass, business-wide view of a broad array of data systems, databases and assets, enabling machine learning, analytics and business intelligence teams to manage an enterprise's entire data landscape through standardized policies and interfaces.

== Introduction

The growth in the amount of business data being collectected continues to accelerate, making utilizing it as much of a challenge as it is an opportunity. While companies are discovering ways to transform their data into products and services that differentiate their business and create new lines of revenue,increasingly they are finding it difficult to fully capitalize on the many datasets they have available to them. At the same time they are coping with the increased cost of managing information that are stored in many, disparate data silos (e.g., cloud object store, on-premise and cloud databases, Hadoop clusters, click-stream and social media feeds). Moreover, maintaining large collections of data adds complexity to their business due to requirements for security, governance, and specialized training. In this light it is not difficult to understand a recent estimation that ~70% of data businesses collected is never utilized. 


== Target Audience 

This paper is directed at professionals involved in both IT Operations and Analytics/Business Intelligence. The recommended framework supplies the requirements to implement {ISVPartner}’s {ISVProductName}, which is certified on {CompanyName} {ProductNameK8s}.

== Strategy

{ISVPartner} has long maintained its position as the market leader in mission critical enterprise applications. Supporting {ISVPartner} with innovation, {CompanyName} has been the market leader for {ISVPartner} applications for over 20 years. {CompanyName} is the trusted and preferred open source platform for {ISVPartner} customers who want to unlock data intelligence, drive innovation and run with the best. As an example of this open source mandate, {ISVPartner} {ISVProductName} is deployed on a Kubernetes-compatible container platform. {CompanyName} {ProductNameK8s} enables you to extend your {CompanyName} Enterprise Linux for {ISVPartner} environment to container-based application delivery.

== Business Problem

Today’s business leaders are under increasing pressure to manage their business with data-driven decisions. This presents a particular challenge for those executives who strive to bring together the right combination of disparate data sources to unlock new value for their business. This difficulty is compounded by the very nature of how data is collected and stored, which results in independent data silos with clear way for mission critical associations to access them.

Compounding on this challenge is the rapidly expanding ecosphere of data analytics and artificial intelligence applications that consume business data, but often in very specific formats. Gone are the days where company datasets were curated based on a common schema with a small number of dataset views to accomodate a handful of enterprise applications. 

In selecting a data management and analytics platform, businesses need to ensure that their software investment can meet the scale of their current and future application/data landscape, as well as meet current and future data governance needs. 

Equally important in this design is the resiliency and scalability of the underlying infrastructure. Experienced leaders know that enterprise-grade software isn't a solid  investment unless it is built on enterprise-grade infrastructure.

== Business Value

{ISVPartner} {ISVProductName} is a modern, fully containerized solution designed to be deployed on enterprise-grade Kubernetes clusters such as {CompanyName} {ProductNameK8s}. For more than 19 years, {ISVPartner} has developed its software on {CompanyName} Linux Enterprise Server (SLES) and {CompanyName} solutions such as our {ProductNameK8s}.

{ISVPartner} {ISVProductName} is built on a next-generation data-aggregation and machine learning model that significantly reduces the need for static data lakes. Instead, {ISVPartner} {ISVProductName} allows for programmatic data extraction and formatting that is executed on the platform where the live data resides. This stands in contrast to the cumbersome practice of running single-use Extract, Load, Transform (ELT) operations. {ISVPartner} {ISVProductName} brings together formatted, refined and cleansed data from multiple sources. The newly integrated dataset can be consumed with {ISVProductName} applications such as the fully integrated Leonardo Machine Learning application suite, or by any other enterprise application that can reach the newly transformed dataset.

{ISVPartner} {ISVProductName} leverages data processing pipelines, which are built from reusable application components. Data pipelines are computational models that are created on the {ISVProductName} platform, but then executed on the data source through the its own, native APIs. Data Pipelines define what data is to be gathered, from which data sources, and how the resultant output should be formatted at the source. Pipelines also specify refinements and cleansing that each stream of data must go through to make it compatible with the other data streams that are being processed by the pipeline. Finally, data pipelines identify which consumer(s) will receive the newly collated data stream. {ISVPartner} {ISVProductName} can persist critical datasets on platform, or simply execute pipelines when required to process and provide the pipeline defined output dataset to the appropriate consumers. This provides advanatages in performance and flexibility, as well as eliminating the need for expensive, scale-limiting data warehouses.

Data pipelines can be created through a graphical user interface to leverage existing data sources such as {ISVPartner} HANA, {ISVPartner} Vora, Apache Spark and Apache Hadoop, as well as all major open and closed source OLTP, OLAP and NoSQL databases.

Before implementing a data-analytics solution, consider the specific problem you are working to solve. Below are some use cases for {ISVPartner} {ISVProductName} that can help you zero in on the type of solution you are pursuing.

== Example Use Cases

This section outlines potential use cases for {ISVPartner} {ISVProductName} built on {CompanyName} Containers as a Service Platform. In general, {ISVPartner} {ISVProductName} excels in pulling data from desparate internal and external data resources to enable insight into very complex analytical problems. The use of machine learning and Big Data analytics platforms ({ISVPartner}, Hadoop, MapR, Cloudera, etc.) require access to large pools of structured and unstructured data in a highly automated, systematic and secure way.

.Fraud Detection
Credit card fraud has become an epidemic, with losses in the billions of dollars. Financial institutions need the ability to create profiles that alert them to probable fraud on massive volumes of transactions. The more information they can cross-reference, the more accurate their models will become. {ISVPartner} {ISVProductName} can pull in transactional data from ERP systems, credit reporting bureaus, email from a Hadoop cluster, social media data and “Dark Web” databases, enabling data scientists to build very precise detection models.

.Manufacturing Equipment Maintenance
Global manufacturers rely on the uptime of their equipment to meet product delivery targets. Unscheduled maintenance or equipment failure can result in lost profits, unacceptable production quality and unmet commitments. Conversely, over-scheduling maintenance activities also impacts cost and output. Manufacturers were early adopters of Internet of Things (IoT) technology for the real-time monitoring of equipment sensors (temperature, vibration, humidity, motor loading, etc.) to gain a better understanding of the state of their environment. What if machine learning (ML) predictive models could be used to optimize maintenance scheduling? {ISVPartner} {ISVProductName} can be used to orchestrate the continuous, end-to-end data flow needed to feed its integrated ML platform to predict impending outages; allowing corrective maintenance to be scheduled before any disruptions occur.

.Customer Affinity Recommendations
E-Commerce sites routinely use various data sources to recommend additional purchases or fine-tune searches to more relevant items. Early attempts were based solely on purchasing behavior at an individual retailer. However, state-of-the-art e-commerce now requires data input from email, social media, browser search data, clickstream data, and credit card reporting sites. {ISVPartner} {ISVProductName} enables you to easily build data pipelines to feed machine learning recommendation models that pass real-time data into an active session on a purchasing website. This information can greatly increase the revenue per transaction metric that is critical to success. 

== Requirements

== Software Architecture

{ISVPartner} {ISVProductName} combines the capabilities of {ISVPartner} {ISVProductName}: data governance and lienage; data preprocessing, integration and cleansing, with the {ISVPartner} Leonardo Machine Learning Foundation. The {isvpartner} {ISVProductName} user interface provides the well known {ISVPartner} Fiori Launch pad combined with the Machine Learning application, “ML Scenario Manager”.  

Figure XYZ shows a high-level view of the architectural components designed to handle the data needs of a wide range of enterprise and machine learning applications. The optional Hadoop cluster can be used as a low latency, high capacity storage and analytics platform for localizing the most critical datasets.

Tenant Applications and Services are the core of {ISVPartner} {ISVProductName}. {ISVPartner} {ISVProductName} provides various tools for the development and administration of custom applications, as well as applications that are accessible through the {ISVPartner} {ISVProductName} application launchpad.

* {ISVPartner} {ISVProductName} Pipelines provide connectors between various {ISVPartner} and external data sources and applications to process them. They are reusable, configurable tool chains to process data from various sources and formats (including CSV files, web services APIs, and {ISVPartner}’s data stores) and can be flexibly designed.

* The {ISVPartner} {ISVProductName} Modeler allows for the creation and configuration of such pipelines through an intuitive graphical user interface.

* The Metadata Explorer provides information about the location, attributes, quality, schema, lineage, and sensitivity of datasets. With this information, you can make informed decisions about which datasets to publish and determine who has access to use or view information about the datasets.

* The Connection Management block enables connections to managed systems or external storage. Services such as Amazon S3, Google Cloud Services, Microsoft Azure (ADL, WASB), data services or Hadoop HDFS can be connected, as well as many different types of databases (Oracle, {ISVPartner} HANA, {ISVPartner} VORA, NoSQL) or business warehouses ({ISVPartner} BW).

== {ISVPartner} Vora Distributed Database
{ISVPartner} Vora is a horizontally scalable, distributed database that can store and process structured data, time-series data (i.e., IoT streams), graph data and semi-structured documents in-memory and/or on disk. {ISVPartner} Vora is only available with {ISVPartner} {ISVProductName}, running in Kubernetes as a fully containerized application.

It can store analytics data in Kubernetes pods, as well as provide a bi-directional Spark2 interface between {ISVPartner} {ISVProductName} and an optionally co-located Hadoop cluster. Like {ISVPartner} {ISVProductName}, {ISVPartner} Vora requires a Kubernetes cluster of at least three worker nodes, and runs alongside {ISVProductName} on the same Kubernetes cluster.

== Persistent Database
This database holds all of the required persistent data required by {ISVPartner} {ISVProductName} (e.g., metadata). This instance is automatically installed, sized, and maintained as part of the overall {ISVProductName} installation process. No special consideration is required.

== Private Container Registry
{ISVPartner} {ISVProductName} utilizes a private container image registry for system, application, and pipeline container images. This can be an enterprise wide registry or one dedicated to the {ISVProductName} cluster. While there are a number of container image registries available, The {CompanyName} Private Registry powered by Harbor is often the best choice for customers who want the best security and management features available combined with the agile development environment that only open source software can provide. {isvpartner} {isvproductname} uses the private registry to store all of the {isvproductname} application components to be deployed in a dev/ops fashion on the Kubernetes cluster as well as data pipeline container images and custom pipeline application artifacts.

== Optional Hadoop Cluster
Optionally, a Hadoop cluster can be built on dedicated nodes and co-located with {ISVPartner} {ISVProductName}. This Hadoop cluster can be used as a local high powered computational/storage medium for {ISVPartner} {ISVProductName} original and uploaded content. The {ISVPartner} {ISVProductName} Spark Extensions are used to interface with the Spark2 environment on the Hadoop cluster for processing and storing data. When utilizing this cluster, {ISVProductName} users can leverage the analytical strengths of {ISVPartner} Vora to analyze and store data in HDFS through the {ISVPartner} {ISVProductName} Vora Spark Extension. {CompanyName} has extensive experience deploying bare-metal and virtualized Hadoop clusters on {CompanyName} Linux Enterprise Server. While this Hadoop cluster uses dedicated nodes, its HDFS storage is built on block storage from the {CompanyName} {ProductName{ProductNameSESshort}} cluster that also serves {ISVPartner} {ISVProductName}. 

== {CompanyName} {ProductNameK8sManager}
{CompanyName} {ProductNameK8sManager} is an integrated software platform that automates the tasks of building, managing and upgrading Kubernetes clusters. It combines the benefits of an enterprise ready operating system with the agility of an orchestration platform for containerized applications such as {ISVPartner} {ISVProductName}. While there are several top-tier Kubernetes platforms in the market, {CompanyName} {ProductNameK8sManager} stands out for its ease of installation and configuration, DevOps integration and enterprise-level operability and scalability.

{companyname} {productnamek8smanager} is capable of managing millions of Kubernetes clusters, of any type that is certified with the CNCF. It can easily manage on premise, public cloud, and edge Kubernetes from a single point of management. For this project we chose the enterprise-grade {CompanyName} {ProductNameK8s} for its flexibility, reliablity, and easy of deployment. The entire deployment consists of the following functions:

*{CompanyName} {productnamek8smanager}*
The {productnamek8smanager} server is a containerized application that can run from any system running a Linux operating system. {productnamek8smanager}'s system requirements very low. Based on being used to manage our {companyname} {productnamek8s} cluster, we can see https://rancher.com/docs/rancher/v2.x/en/installation/requirements/#hardware-requirements[here] that two vCPUs and eight GB of memory are recommended. {CompanyName} {ProductNameK8sManager} performs the deployment, management, and software upgrades for the {productnamek8sshort} cluster. If available, it is recommended to deploy the {productnamek8smanager} server in an existing Kubernetes cluster for the best availability and reliability.

*Kubernetes Master Nodes*
The {ProductNameK8s} master nodes maintain the containerized Kubernetes control plane services and etcd configuration store. While three or more master nodes (always an odd number) are required for high availability of the Kubernetes control plane, a single master node is acceptable for demonstration purposes. There exists the granularity in the master node deployment to separate the control-plane and etcd functionalities onto separate nodes. With specific considerations, this can improve the resiliency and performance of the etcd configuration store; though most enterprise class {productnamek8sshort} clusters keep these services together on three master nodes.

*Kubernetes Worker Nodes*
The {ProductNameK8s} worker nodes run the {ISVPartner} {ISVProductName} application containers. {ISVPartner} {ISVProductName} requires a minimum of three Kubernetes worker nodes (four worker nodes for production). Additional worker nodes can be added to a Production {ProductNameK8s} cluster non-disruptively. The system requirements vary based on the needs of each customer but general sizing guidelines can be reviewed at https://help.sap.com/viewer/1f833eab23244ef2ad66fe982dd14873/2.7.latest/en-US/adb8e6505e0c414faf57138b4cc6f075.html[T-Shirt Sizes for SAP Data Intelligence] and https://help.sap.com/viewer/835f1e8d0dde4954ba0f451a9d4b5f10/3.0.latest/en-US/633d429ff69441ae81fe57d912397903.html[Sizing Guide for SAP Data Intelligence]

== Optional {CompanyName} {productnamepaas} 
{CompanyName} {productnamepaas} is a modern application delivery environment used to bring an advanced cloud-native DevOps experience to container-based infrastructure. {CompanyName}’s implementation is based on the open source Project Eirini, which uses Kubernetes to orchestrate application containers while maintaining the Cloud Foundry user experience. This Platform as a Service (PaaS) environment is used by developers to streamline lifecycle management of traditional and cloud-native applications. Together, these technologies accelerate innovation, improve IT responsiveness, and maximize return on investment. 

== Storage Architecture 
The storage layer of this solution leverages the Software Defined Storage capabilities of {CompanyName} {ProductName{ProductNameSESshort}} ({ProductNameSESshort}). {ProductNameSESshort} is a commercially supported distribution of the Ceph enterprise grade, scale-out storage solution. {ISVPartner} requires a certified solution for storage that supports Rados Block Devices as well as Dynamically Provisioned Volumes. (See {ISVPartner} Note 2686169 for certified storage options.)

Ceph is a scale-out, distributed object store that provides excellent performance, scalability and reliability. In most use cases, clients use Linux kernel libraries to read and write object and block data directly to/from a storage node in the {ProductNameSESshort} cluster. {ProductNameSESshort} also provides gateway options to support data access via iSCSI, NFS, S3 and Swift protocols. The storage capacity of the {ProductNameSESshort} solution can be expanded easily by integrating additional storage nodes into the cluster. Existing storage nodes will take care of redistributing the data to the newly added nodes without interrupting the availability of storage services to the clients.

{ProductNameSESshort} provides a reliable, scalable storage layer for the complete solution, which supports: 
* Dynamically provisioned block storage volumes to the pods running on {CompanyName} {ProductNameK8s}
* (Optionally) Block storage volumes for the co-located Hadoop cluster nodes, if configured
* Object storage through an S3-API-compatible interface, for additional data storage and backups

.Dynamically Provisioned Storage Volumes
In addition to providing block storage to the optional Hadoop cluster, a pod running on {ProductNameK8s} can gain access to dynamically provisioned Kubernetes persistent volumes (PV)
persistent volume claims (PVC) through the RBD (Rados Block Device) CSI (Container Storage Interface) storage class. Persistent volumes are created as block devices in the supporting {ProductNameSESshort} cluster. {ProductNameK8s} uses persistent volume claims (PVCs) to obtain dynamically provisioned persistent volumes through the Software Defined Storage mechanisms in {ProductNameSESshort}. When a PVC is removed, the persistent volume and its associated block storage device in {ProductNameSESshort} are automatically removed.

== Software and Systems Management
While {ISVPartner} {ISVProductName} doesn’t require an external {ISVPartner} HANA instance in order to function, most users of this solution will be attaching to an existing HANA database to build their data pipelines. After assembling this combined data pipeline and writing to your HANA database, you can take advantage of *{ISVPartner} Advanced Analytics Processing* capabilities, including machine learning/predictive analytics, spatial intelligence (location awareness) and streaming data processing. The scaleout capabilities of {ISVPartner} HANA support rapid data growth, but it is important to have a dependable method of keeping your {ISVPartner} HANA servers up to date. *{CompanyName} Manager* can mirror {ProductNameK8s} installations and update packages to help enforce consistency across your organization. {CompanyName} Manager can also analyze the container images in your private container registry as well as containers running on your {CompanyName} {ProductNameK8s} for known vulnerabilities, outstanding patches, or pending package updates. {CompanyName} Manager enables you to efficiently manage a set of Linux
systems and keep them up to date. 

An {ISVPartner} HANA scale-out setup offers these benefits:

*Reduced Complexity of Managing {ISVPartner} HANA Environments*
* Ensure consistent management of {ISVPartner} HANA and all other cluster systems.
* Manage your data environment across physical, virtual and cloud environments.
* Manage your channels effectively.
*Create/Manage Development, QA and Production Channels*
* Add and manage third-party channels.
* Simplify compliance.
*Audit the Patch Status for {ISVPartner} HANA and Subsystems*
* Track the configuration changes and make sure all administrators have the right authority for changes.
* Slash costs of ownership.
*Automate System Management Tasks for {ISVPartner} HANA and All Other Subsystems*
* Leverage a single, web-based interface to see the status of all your servers.
* Use your resources effectively.

== Summary
{CompanyName} {ProductNameK8s} is an excellent environment for creating an {ISVPartner} {ISVProductName} implementation. This composable infrastructure enables you to define appropriate hardware from software descriptions. This means you can easily scale, adjust and customize your environment to fit your needs as you move from a proof-of-concept toward a production environment. {CompanyName} {ProductNameK8s} is an enterprise Kubernetes container platform that provides software infrastructure for not only the {ISVPartner} {ISVProductName} software described in this reference, but also the data analytics applications you will build to ingest and manage your data. All of the software environments in this reference architecture are supported products and have been tested to work together on industry-standard x86-64 gear.

Join the best. Run your {ISVPartner} solutions on {CompanyName}
