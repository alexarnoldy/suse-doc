
{CompanyName} {ProductName} is an intelligent software-defined storage solution, powered by Ceph technology, which enables you to transform your enterprise storage infrastructure. While a software-defined approach may seem new, the logic within enterprise storage devices has always been written in software. Itâ€™s only been in the last few years that hardware has progressed enough so that enterprise storage software and dedicated hardware can now be separated. IT organizations now gain a simple to manage, agile infrastructure approach yielding increased speed of delivery, durability and reliability. This helps to accelerate innovation, reduce costs and alleviate proprietary hardware lock-in by transforming your enterprise storage infrastructure with a truly open, and unified intelligent software-defined storage solution.

Being a single, truly unified, software-defined storage cluster that provides applications with object, block and file system storage providing ubiquitous and universal access to your legacy and modern applications and automated durability of your data with high availability and disaster recovery options. Some key features are:

* Unlimited scalability with a distributed storage cluster designed to scale to thousands of nodes and multi-hundred petabyte environments and beyond to meet your growing data requirements.
* Highly redundant storage infrastructure design maximizes application availability with no single points of failure.
* Self-healing capabilities minimize storage administration involvement and optimize data placement, enabling rapid reconstruction of redundancy, maximizing system resiliency and availability.Self-healing capabilities minimize storage administration involvement and optimize data placement, enabling rapid reconstruction of redundancy, maximizing system resiliency and availability.
* Utilize commodity off-the-shelf hardware that is at minimum 30 percent less expensive than average capacity optimized solutions to drive significant CAPEX savings.
* Automated re-balancing and optimized data placement with an easy to manage intelligent solution that continuously monitors data utilization and infrastructure without any manual intervention and without growing IT staff
* Included with the solution are the following supported protocols:

** Native
*** RBD (Block)
*** RADOS (Object)
*** CephFS (With multiple active MDS Servers)
*** S3 & SwiftRBD (Block)

** Traditional
*** iSCSI
*** NFS
*** CIFS/SMB

As noted above Ceph supports both native and traditional client access. The native clients are aware of the storage topology and communicate directly with the storage daemons, resulting in horizontally scaling performance. Non-native protocols, such as ISCSI, S3, and NFS require the use of gateways. While these gateways may be thought of as a limiting factor, the ISCSI and S3 gateways can scale horizontally using load balancing techniques. 

[[img-SES]]
.Ceph Archtecture
image::suse-enterprise-storage-release-graphic.png[SUSE Enteprise Storage, 640, 480]

Descriptions of core components, roles and other needed services are noted below:

Cluster Networking::
The network environment where you intend to run Ceph should ideally be a bonded set of at least two network interfaces that is logically split into a public part and a trusted internal part using VLANs. The bonding mode is recommended to be 802.3ad if possible to provide maximum bandwidth and resiliency.  The public VLAN serves to provide the service to the client nodes, while the internal part provides for the authenticated Ceph network communication. The main reason for this is that although Ceph provides authentication and protection against attacks once secret keys are in place, the messages used to configure these keys may be transferred openly and are vulnerable. 

Administration Node::
The Admininstration Node is a central point of the Ceph cluster because it manages the rest of the cluster nodes by querying and instructing their Salt minion services. It usually includes other services as well, for example the Ceph Dashboard (formerly known as openATTIC) web interface with the Grafana dashboard backed by the Prometheus monitoring toolkit.

Ceph Monitor::
Ceph Monitor (often abbreviated as MON) nodes maintain information about the cluster health state, a map of all nodes and data distribution rules. If failures or conflicts occur, the Ceph Monitor nodes in the cluster decide by majority which information is correct. To form a qualified majority, it is recommended to have an odd number of Ceph Monitor nodes, starting with at least three of them.

Ceph Manager::
The Ceph manager (MGR) collects the state information from the whole cluster. The Ceph manager daemon runs alongside the monitor daemons. It provides additional monitoring, and interfaces the external monitoring and management systems. The Ceph manager requires no additional configuration, beyond ensuring it is running.

Ceph OSD::
A Ceph OSD is a daemon handling Object Storage Devices which are a physical or logical storage units (hard disks or partitions). Object Storage Devices can be physical disks/partitions or logical volumes. The daemon additionally takes care of data replication and rebalancing in case of added or removed nodes. Ceph OSD daemons communicate with monitor daemons and provide them with the state of the other OSD daemons. 

Optional Roles::
* Metadata Server
** The metadata servers (MDS) store metadata for the CephFS. By using an MDS you can execute basic file system commands such as ls without overloading the cluster
* Object Gateway
** The Ceph Object Gateway provided by Object Gateway is an HTTP REST gateway for the RADOS object store. It is compatible with OpenStack Swift and Amazon S3 and has its own user management
* NFS Ganesha
** NFS Ganesha provides an NFS access to either the Object Gateway or the CephFS. It runs in the user instead of the kernel space and directly interacts with the Object Gateway or CephFS
* iSCSI Gateway
** iSCSI is a storage network protocol that allows clients to send SCSI commands to SCSI storage devices (targets) on remote servers 

Additional Network Infrastructure Components / Services::
* Domain Name Service (DNS) - an external network-accessible service to map IP Addresses to hostnames for all cluster resource nodes
* Network Time Protocol (NTP) - an external network-accessible service to obtain and synchronize system times to aid in timestamp consistency. It is recommended to point all resource nodes as a physical system/device that provides this service.
* Software Update Service - access to a network-based repository for software update packages. This can be accessed directly from each node via registration to the http://scc.suse.com[{CompanyName} Customer Center] (SCC) or from local servers running a SUSE https://www.suse.com/documentation/sles-12/singlehtml/book_smt/book_smt.htm[Subscription Management Tool] (SMT) instance. As each node is deployed, it can be pointed to the respective update service and update notification and applicate will be managed by the configuration management web interface. 

