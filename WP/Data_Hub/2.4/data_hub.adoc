//
// File: data_hub.adoc
// Desc: WP and RA for SAP Data Hub on SUSE CaaS Platform
//

// NOTE: This is overridden via attributes passed on the command line (See: Makefile)
// :Author: Alex Arnoldy
// :AuthorEMail: alex.arnoldy@suse.com
//:Author: Brian Fromme
//:AuthorEMail: brian.fromme@suse.com

:ISVPartner: SAP
:ISVSolution: Data Hub

:CompanyName: SUSE
:ProductName: CaaS Platform
:StorageName: Enterprise Storage

:IHVPartner: IHV
:IHVPlatform: general

//:IHVPartner: Dell
//:IHVPlatform: PowerEdge
//:IHVPlatformModel: R640
//:IHVPlatformBMC: iDrac

//:IHVPartner: HPE
//:IHVPlatform: ProLiant
//:IHVPlatformComposer: Composer
//:IHVPlatformModel: DL360
//:IHVPlatformBMC: iLO

//:IHVPartner: HPE
//:IHVPlatform: Synergy
//:IHVPlatformComposer: Composer
//:IHVNetwork: FlexFabric
//:IHVPlatformModel: SY480
//:IHVPlatformBMC: iLO

//:IHVPartner: Lenovo
//:IHVPlatform: ThinkSystem
//:IHVPlatformModel: SR530
//:IHVPlatformBMC: xClarity Controller

:xrefstyle: short

ifdef::env-github[]
:imagesdir: https://github.com/bwgartner/suse-doc/blob/master/WP/Data_Hub/2.4/images/src/png/
endif::[]

= {ISVPartner}^(R)^ {ISVSolution} on {CompanyName}^(R)^ Container as a Service Platform
{Author}, {CompanyName} < {AuthorEMail} >

// == Revision History

// .Revisions
// [options="header",cols=",,"]
// |=======================
// |Version |Date         |Comments
// |0.5     |01/15/19     |Initial draft
// |0.6     |02/11/19     |Collaborative draft
// |1.0     |03/05/19     |First draft to be shared with {IHVPartner} as {CompanyName} Company Confidential
// |1.1     |03/31/19     |Proposed {IHVPartner} hardware config - shared as {CompanyName} Company Confidential
// |1.2     |04/15/19     |Merged templates to allow easy generation of various platforms.
// |=======================

// _*DRAFT COPY - DO NOT PUBLISH*_

<<<
== Introduction
Enterprise data is exploding and is both a challenge and an opportunity. Companies are discovering ways to transform their data into services that help differentiate the business and create new lines of revenue.  Unfortunately, managing and fully utilizing the information stored in data silos (e.g. cloud databases, Hadoop clusters, social media feeds) has become incredibly complex due to requirements for security, governance, and specialized training.  https://www.sap.com/products/data-hub.html[{ISVPartner} {ISVSolution}] provides a GUI-based business-wide view of a broad array of data systems, databases, and assets to enable your analytics and business intelligence teams to manage your entire data landscape through an intuitive “single pane of glass”.    In what is likely to become a trend, {ISVPartner} mandates that this application is deployed on a Kubernetes compatible container platform. https://www.suse.com/products/caas-platform/[{CompanyName} {ProductName}] enables you to extend your {CompanyName} Enterprise Linux for {ISVPartner} environment to container-based application delivery.

=== Target Audience
This discussion will be of interest to professionals involved in both IT Operations and Analytics/Business Intelligence.   The recommended framework supplies the requirements to implement {ISVPartner}’s {ISVSolution} that is certified on {CompanyName} {ProductName}.

=== Business Problem
Today's business leaders are under increasing pressure to drive their business with data-driven decisions. This presents a particular challenge for those executives who strive to bring together the right combination of disparate data sources to unlock new value for their business.

This difficulty is compounded by the very nature of how data is collected and stored, which results in independent data silos with no easy way to make critical associations across them. These data silos may be stored geographically close together, or far apart. Some may be built with on-premise resources, and some housed in one or more public clouds. Valuable data is often found in structured and unstructured databases, Hadoop data lakes, data warehouses, and even in text files. Gaining new insights into potential customers and business opportunities may involve nearly all of the data silos a company has available to it.

[[img-Data_Pipeline_Funnel]]
.{ISVPartner} {ISVSolution} Data Pipeline Funnel
image::Data_Funnel_Pipeline.png[{ISVPartner}_Data_Hub_Data_Pipeline_Funnel, 241, 300]

What's more, finding a business application to perform the task is not even the hardest part. Businesses need to ensure their data-analysis software investment can meet the scale of their current, as well as their future application/data landscape, plus enforce data governance. Equally as important is the resiliency and scalability of the underlying infrastructure. Experienced leaders know that enterprise-grade software is a poor investment if it is not built on enterprise-grade infrastructure.

=== Business Value
{ISVPartner} {ISVSolution} is a containerized solution designed to be deployed on enterprise-grade Kubernetes clusters such as {CompanyName} {ProductName}.  For more than 17 years {ISVPartner} has developed their software on {CompanyName} Linux Enterprise Server (SLES) and {CompanyName} solutions such as {ProductName}.

NOTE: See [.underline]*SAP Note 2693555* for certified systems.

{ISVPartner} {ISVSolution} is built on a next-generation data-aggregation model that does away with the need for expensive data warehouses. Instead, {ISVPartner} {ISVSolution} allows data extraction and formatting to be done on the platform where the data resides. This is in contrast to the current practice of using cumbersome, single-use Extract, Load,  Transform (ELT) operations that are used to populate data warehouses; {ISVPartner} {ISVSolution} provides formatted, refined and cleansed data from multiple sources directly to the data consumers.

{ISVPartner} {ISVSolution} leverages data pipelines, which are built from reusable application components. Data pipelines are computational models that are executed natively on the data source. They define what data should be gathered from which sources, and how that data should be formatted at the source. Pipelines also specify what refinements and cleansing each stream of data should go through to make it compatible with the other data streams in the pipeline. Finally, they identify to which consumer or consumers the collated data should be sent. Since {ISVPartner} {ISVSolution} does not need to persist data, it completely eliminates the need for expensive, scale-limiting data warehouses.

Data pipelines can be created through a graphical user interface to leverage existing data sources such as {ISVPartner} HANA, {ISVPartner} Vora, Apache Spark, and Apache Hadoop; as well as all major open and closed source OLTP, OLAP and NoSQL databases.

Before implementing a data-analytics solution, consider the specific problem you are working to solve. Below are some use cases for {ISVPartner} {ISVSolution} that may help to zero in on the type of solution you are pursuing.

== Example Use Cases
This section will outline a few potential use cases for SAP Data Hub built on SUSE Containers as a Service Application Platform.   In general, SAP Data Hub excels in pulling information from multiple kinds of internal and external data resources to enable insight into very complex analytical problems.   The use of machine learning and Big Data analytics platforms like SAP, Hadoop, MapR, Cloudera, etc. require access to large pools of unstructured data in a highly automated, systematic and secure way.

=== Fraud Detection

Credit card fraud has become an epidemic with losses in the billions of dollars.  Financial institutions need the ability to create profiles that alert to probable fraud on large volumes of transactions.   The more information they can cross-reference, the more accurate their models will become.  SAP Data Hub can pull in transactional data from ERP systems, credit reporting bureaus, email from a Hadoop cluster, social media data, and “Dark Web” databases to enable data scientist to build very precise detection methodologies.

=== Manufacturing Equipment Maintenance

Global manufacturers rely on the uptime of their equipment to meet product delivery targets.   Unscheduled maintenance or equipment failure can result in lost profits, poor quality, and unmet commitments.   Conversely, over-scheduling maintenance activities also impacts cost and output.   Manufacturers were early adopters of Internet of Things (IOT) technology for the real-time monitoring of equipment sensors (e.g. temperature, vibration, humidity, motor loading, etc.)  to have a better understanding of the state of their environment.   What if predictive models and machine learning (ML) could be used to optimize maintenance scheduling?  SAP Data Hub can be used to orchestrate the end-to-end data flow to feed an ML platform to predict impending outages and schedule corrective maintenance before any disruptions.

=== Customer Affinity Recommendations

E-Commerce sites routinely use various data sources to try to recommend an additional purchase or fine-tune your search to more relevant items.   Early attempts were based solely on purchasing behavior at an individual retailer, but state of the art now requires data input from email, social media, browser search data, clickstream data, and credit card reporting sites.   {ISVPartner} {ISVSolution} enables you to easily build this pipeline to feed real-time recommendations into an active session on a purchasing website. This information can greatly increase the revenue per transaction metric that is critical to success.


== Requirements
As an IT organization evaluates solutions to the data growth and migration challenge, key requirements will be defined. Below are some typical requirements to consider as you experiment and evaluate software for your analytics applications.

.Data Analytics Solution Requirements
[options="header",cols=","]
|===
|Requirement Type      |Details
|Existing Data Stores  |Access data from a variety of data sources, including Hadoop data lakes, object stores, databases, and data warehouses, both in the cloud and on-premise.
|                      |Perform data transformations, data quality, and data preparation processes.
|                      |Define data pipelines and streams.
|                      |Embed and productize scripts, programs, and algorithms of the Data Scientist.
|                      |Productize open libraries or ML algorithms in one framework.
|Distributed Data Processing|Distribute computational tasks to the native environments where the data reside.
|                      a|Remote process scheduling: +

                          * {ISVPartner} Business Warehouse process chains +
                          * {ISVPartner} Data Services dataflows +
                          * {ISVPartner} HANA Smart Data Integration FlowGraph

|Governance            |Establish and manage zones in a landscape with attached policies and services levels.
|                      |Security and Access Control capabilities.
|Orchestration         |Workflow creation of operations and processes across the landscape with monitoring and analysis capabilities.
|                      |Execution of end-to-end data processes, starting with the ingestion of data into the landscape (e.g. the data lake), including data processing, and leading up to the delivery or integration of the resulting data into enterprise processes and applications.
|Data Ingestion and Processing|Data integration, cleansing, enrichment, masking and anonymization.
|Data Discovery        |Data Profiles for Big Data Sets showing quality and comprehensive structure information.
|                      |Ability to crawl, discover, and tag data elements.
|                      |Expose discovered data for further usage.
|Scalability           |Scalable Architecture, from small to big, test to production deployment.
|Deployment            |Easy deployment, using a proven-to-work combination of the components.
|Fault Tolerance       |Single component error will not lead to whole system unavailability.
|Ease of Management/Ops|Reduced complexity for solution management.
|Physical Footprint    |Compact solution that works within your existing infrastructure models.
|Flexibility           |Flexible building block approach allows sizing according to customer needs.
|Security              |Solution provides the means to secure customer infrastructure.
|High performance      |Best practices are designed into the solution to ensure the best performance results.
|===


With a solid understanding of your requirements, you can begin to design the solution.  The following section will outline the key concepts in the software architecture of the {ISVPartner} {ISVSolution} reference configuration.

== Software Architecture

=== {ISVPartner} {ISVSolution}
{ISVPartner} {ISVSolution} offers data management capabilities to help customers manage their growing amount of data. This solution combines data governance, management of data pipelines and data integration using a single visual interface and without the need for moving data into a central data warehouse.  <<img-Solution_Architecture>> shows a high-level view of the architectural components designed to handle a wide range of enterprise applications scenarios.  The optional Hadoop cluster can be used as the main software platform for handling the composition of application data.

[[img-Solution_Architecture]]
.{ISVPartner} {ISVSolution} Architecture
image::SAP_HANA_Architecture.png[{ISVPartner}_Data_Hub_Architecture, 800, 260]

Tenant Applications and Services::
Tenant Applications and Services are the core of {ISVPartner} {ISVSolution}. {ISVPartner} {ISVSolution} provides various tools for development and administration, as well as applications that are accessible through the {ISVPartner} {ISVSolution} application launchpad. {ISVPartner} {ISVSolution} Pipelines are the connectors between the various {ISVPartner} {ISVSolution} data sources. They provide reusable, configurable operations to process data from the various sources, including CSV files, web services APIs as well as {ISVPartner}’s own data stores and can be flexibly designed. The {ISVPartner} {ISVSolution} Modeler allows the creation and configuration of such pipelines through a graphical user interface. The Metadata Explorer provides information about the location, attributes, quality, and sensitivity of data. With this information, you can make informed decisions about which datasets to publish and determine who has access to use or view information about the datasets.  The Connection Management block enables connections to managed systems or external storage. Services such as Amazon S3, Google Cloud Services, Microsoft Azure (ADL, WASB), data services, or Hadoop HDFS can be connected, as well as databases (Oracle, {ISVPartner} HANA, {ISVPartner} VORA) or business warehouses ({ISVPartner} BW).


{ISVPartner} Vora Distributed Database::

{ISVPartner} Vora is a horizontally scalable, distributed database which can store and process structured data, time-series data (i.e. IoT streams), graph data and semi-structured documents in-memory and/or on disk. {ISVPartner} Vora is only available with {ISVPartner} {ISVSolution}, running in Kubernetes as a fully containerized application. It can store analytics data in Kubernetes pods as well as provide a bi-directional Spark2 interface between {ISVPartner} {ISVSolution} and an optionally co-located Hadoop cluster. Like {ISVPartner} {ISVSolution}, Vora requires a Kubernetes cluster of at least three Worker Nodes, but runs alongside Data Hub on the same cluster.

Persistent Database::
This database holds all of the required persistent data required by {ISVPartner} {ISVSolution} (e.g. metadata)  This instance is automatically installed, sized, and maintained as part of the overall Data Hub installation process.   No special consideration is required.


Container Registry::
{ISVPartner} {ISVSolution} requires a Docker repository for container images. This can be a publicly accessible site or a private collection of workload images. Other public or private registry sites can be used to provide files like Helm charts to deploy complete services. Although the private Docker registry is not part of the {CompanyName} {ProductName}, you can either build an on-premise instance using the Containers Module Add-on included with {CompanyName} Linux Enterprise Server for {ISVPartner} along with the {CompanyName} Portus (http://port.us.org) package or deploying this as a container directly on {CompanyName} {ProductName}.  Portus is an open source on-premise authorization service that allows users to administrate and secure their Docker registries with fine-grained control.

Optional Hadoop Cluster::
An optional Hadoop cluster can be built on dedicated nodes and co-located with {ISVPartner} {ISVSolution}. This associated Hadoop Data Lake can be used as a local computational/storage medium for {ISVPartner} {ISVSolution} original and uploaded content. The {ISVPartner} {ISVSolution} Spark Extensions are used to interface with the Spark2 environment on the Hadoop cluster for processing and storing data.  When utilizing this cluster, Data Hub users can leverage the analytical strengths of {ISVPartner} Vora to analyze and store data in HDFS through the {ISVPartner} {ISVSolution} Vora Spark Extension. {CompanyName} has extensive experience deploying bare-metal and virtualized Hadoop clusters on {CompanyName} Linux Enterprise Server. While this Hadoop cluster uses dedicated nodes, its HDFS storage is built on block storage from the https://www.suse.com/products/suse-enterprise-storage/[{CompanyName} Enterprise Storage] cluster that also serves {ISVPartner} {ISVSolution}.

// TRANSITION PARAGRAPH NEEDED?

=== {CompanyName} {ProductName}
{CompanyName} {ProductName} is an integrated software platform which automates the tasks of building, managing and upgrading Kubernetes clusters. It combines the benefits of an enterprise-ready operating system with the agility of an orchestration platform for containerized applications such as {ISVPartner} {ISVSolution}.

While there are several top-tier Kubernetes offerings in the market, {CompanyName} {ProductName} stands out for its ease of installation and configuration, DevOps integration (via https://www.suse.com/products/cloud-application-platform/[{CompanyName} Cloud Application Platform]), and enterprise level of operability and scalability.

One of the biggest challenges for Kubernetes operators is matching the scalability of the node level infrastructure with that of the overlaying container infrastructure. Inconsistently applied software changes, as well as node configuration drift, create ticking time bombs in production Kubernetes clusters.

{CompanyName} {ProductName} (<<img-CaaSP_Detailed_Architecture>>) resolves these problems with a combination of {CompanyName} MicroOS as the container host operating system and Salt for configuration management. {CompanyName} MicroOS is a mission-specific derivative of {CompanyName} Linux Enterprise Server (SLES). While MicroOS leverages the same codebase and packages, its implementation ensures that software changes are applied atomically and within a snapshot-protected environment. The combination of MicroOS and Salt guarantees that all nodes in a cluster are always in a known and consistent state. The troubleshooting nightmares of discovering a single node with a partially-failed configuration or software change are a thing of the past.

[[img-CaaSP_Detailed_Architecture]]
.{CompanyName} {ProductName} Architecture
image::CaaSP_Detailed_Architecture.png[CaaSP_Detailed_Architecture, 814, 440]

A {CompanyName} {ProductName} (<<img-CaaSP_Nodes>>) consists of the following node types:

Administration Node::
The Administration Node of the {CompanyName} {ProductName} manages the deployment of the cluster and runs central services like:
* *Velum*: Web-UI dashboard used to administer the cluster.
* *Salt Master*:  Manages the configuration of the cluster nodes.
* *MariaDB Database*: Stores Velum data and Salt master daemon events
* *Dex Identity Service*: Provides user authentication and a robust role-based access control (RBAC) system.

Kubernetes Master Nodes::
The {ProductName} Master Nodes maintain the Kubernetes control plane services. These services run as containers on the Master Nodes. While three or more Master Nodes (always an odd number) are required for high availability of the Kubernetes control plane, a single Master Node is acceptable for demonstration purposes.

Kubernetes Worker Nodes::
The {ProductName} Kubernetes Worker Nodes run the {ISVPartner} {ISVSolution} application containers. {ISVPartner} {ISVSolution} requires a minimum of three Kubernetes Worker Nodes (four worker nodes for production) and {CompanyName} currently supports {ProductName} clusters of up to 150 nodes. Additional Worker Nodes can be added to a Production {ProductName} cluster non-disruptively.

NOTE: {ISVPartner} specifies that each worker node must have a least 8 cores and 64GB of main memory.

[[img-CaaSP_Nodes]]
.{CompanyName} {ProductName} Node Configuration
image::CaaSP_Nodes.png[CaaSP_Nodes, 200, 319]

Optional {CompanyName} Cloud Application Platform::
https://www.suse.com/products/cloud-application-platform[SUSE Cloud Application Platform] is a modern application delivery environment used to bring an advanced cloud native DevOps experience to container-based infrastructure.    SUSE's implementation is based on the open source https://www.cloudfoundry.org/project-eirini/[Project Eirini] which uses Kubernetes to orchestrate application containers while maintaining the Cloud Foundry user experience. This Platform as a Service (PaaS) environment is used by developers to streamline lifecycle management of traditional and cloud native applications.  Together, these technologies accelerate innovation, improve IT responsiveness, and maximize return on investment.


// BG:  ADD HOW YOU CREATE ANALYTICS (SIDECAR TYPES OF) WORKLOADS WITH CAP.  DEV TEAM/SA PEERS
// TRANSITION PARAGRAPH NEEDED?

== Storage Architecture
The storage layer of this solution leverages the Software Defined Storage capabilities of {CompanyName} Enterprise Storage (SES). SES is a commercially supported distribution of the Ceph enterprise-grade, scale-out storage solution.  SAP requires a certified solution for storage that supports Rados Block Devices as well as Dynamically Provisioned Volumes (See [.underline]*SAP Note 2686169* for certified storage options)

NOTE: SAP Data Hub 2.x no longer supports the NFS protocol (See [.underline]*SAP Note 2712050*)


Ceph is a scale-out, distributed object store which provides excellent performance, scalability, and reliability. In most use cases clients use Linux kernel libraries to read and write object and block data directly to/from a storage node in the SES cluster. SES also provides gateway options to support data access via iSCSI, NFS, S3, and Swift protocols.

The storage capacity of the SES solution can be expanded easily by integrating additional storage nodes to the cluster. Exiting storage nodes will take care of redistributing the data to the newly added nodes without interrupting the availability of storage services to the clients.

SES provides a reliable, scalable storage layer for the complete solution that supports:

* Dynamically provisioned block storage volumes to the pods running on {CompanyName} {ProductName}.
* (Optionally) Block storage volumes for the co-located Hadoop cluster nodes, if configured.
* Object storage through an S3-API compatible interface for additional data storage and backups.


Dynamically Provisioned Storage Volumes::
In addition to providing block storage to the optional Hadoop cluster, a pod running on {ProductName} can gain access to dynamically provisioned Kubernetes persistent volumes (PV) through Kubernetes persistent volume claims (PVC). Persistent volumes are created as block devices in the supporting SES cluster. {ProductName} uses persistent volume claims (PVC)s to obtain dynamically provisioned persistent volumes through the Software Defined Storage mechanisms in SES. When a PVC is removed, the persistent volume and its associated block storage device in SES are automatically removed.

=== Software and Systems Management
While {ISVPartner} {ISVSolution} doesn't require an external SAP HANA instance to function, most users of this solution will be attaching to an existing HANA database to build their data pipelines.   After assembling this combined data pipeline and writing to your HANA database, you can take advantage of https://www.sap.com/products/hana/features/advanced-analytics.html[{ISVPartner} Advanced Analytics Processing] capabilities including machine learning/predictive analytics, spatial intelligence (location awareness), and streaming data processing.  The scale-out capabilities of  SAP HANA support rapid data growth, but it is important to have a dependable method of updating your {ISVPartner} HANA servers.  {CompanyName} Manager enables you to efficiently manage a set of Linux systems and keep them up-to-date. The benefits in a {ISVPartner} HANA scale-out setup are:

Reduced Complexity of Managing {ISVPartner} HANA Environments::
* Ensure consistent management of {ISVPartner} HANA and all other cluster systems.
* Manage your data environment across physical, virtual and cloud environments.
* Manage your channels effectively.

Create/Manage Development, QA and Production Channels::
* Add and manage third-party channels.
* Simplify compliance.

Audit the Patch Status for {ISVPartner} HANA and Subsystems::
* Track the configuration changes and make sure all administrators have the right authority for changes.
* Slash costs of ownership.

Automate System Management Tasks for {ISVPartner} HANA and All Other Subsystems::
* Leverage a single web-based interface to see the status of all your servers.
* Use your resources effectively.

== Hardware Architecture

This reference defines a private proof-of-concept cluster.  Other guides will address cloud-based and production environments.  The proof-of-concept cluster is a starting point for prototyping real-world applications that are meant to go into production.  As such, a proof-of-concept cluster can easily be grown into a production environment.  Further, a private deployment is useful for secure, in-house prototyping, but is not restricted from using cloud-based applications and data.  SAP Data Hub can manage data from locations both behind the firewall and in the cloud.

The backbone of the application environment is the Kubernetes cluster, as implemented in the {CompanyName} {ProductName} software.  As described in the architecture section above, this cluster can be implemented on low-cost, commodity hardware, typically 1U racked boxes with 2-socket processors.  The application environment reflects a key aspect of growth from prototyping to production.  In a PoC, there are typically few application containers, as developers are gaining experience building, deploying, and managing code as containers on the new software environment.  As such, there are less demands on the hardware for resources, particularly main memory.  Memory can easily be expanded, as the cluster needs to grow.  Processor speed, on the other hand, is something to carefully consider.  It is not as easy to swap processors, so choosing a medium to high-end Xeon processor with many cores is a good idea.

It is recommended to start with a minimum 2-socket Xeon processor with 8 to 12 cores per CPU.  Memory can start at 64GB.  Local storage on the cluster is used primarily for the operating system and any application temporary data.  A common choice is a RAID1 configuration of operating system disks, each with a minimum of 256GB.  SSD is the recommended choice, as it speeds operations and doesn't add considerable cost to the cluster in this case.  Networking is another key performance area, so 10GbE NICs are a minimum recommendation.  You can start with 1 NIC, but that is a bare minimum and 2 or more NICs will be needed for specific network requirements, based on your application needs.  In particular, high-bandwidth data pipelines will move lots of data to the cluster storage and can easily saturate a single NIC.  Consider mapping 1 high-speed NIC to those storage requirements.

As described in the {CompanyName} {ProductName} section above, a minimum of 5 nodes will be required: 1 as the administration node, 1 master node, and 3 worker nodes.  Any https://www.suse.com/yessearch/[{CompanyName} YES] certified {IHVPartner} platform can be used for the physical nodes of this deployment, as long as the certification refers to the major version of the underlying {CompanyName} operating system required by the {CompanyName} {ProductName} release.

One key benefit of this data analytics implementation is that {IHVPartner} industry-standard servers can fulfill each of the resource node's computational and additional storage needs. To reduce the time spent on hardware specification for an initial proof of concept implementation, the hardware should be general purpose and allow for a wide range of configuration options. The following sections detail attributes of the {IHVPartner} overall portolio and some specific recommendations of platform models.

include::{IHVPartner}.adoc[]

=== Compute

The following considerations for the system platforms should be emphasized:

* Ensure that all similar system devices are consistent and up-to-date with regard to BIOS/uEFI/device firmware versions to reduce potential troubleshooting issues later.

* Reset the BIOS setup configuration to the default setting to have a known baseline configuration to provide consistency.

* If possible, setup RAID1 mirroring on the storage controller across a pair of drives for the operating system installation.

=== Storage
Discussed under the Storage Architecture section, the storage layer of this solution leverages the Software Defined Storage capabilities as provided by {CompanyName} {StorageName}.

{ISVPartner} {ISVSolution} and {CompanyName} {ProductName} are the base framework for a data analytics environment.  The data analytics you execute will be defined by a set of application containers that run on the {CompanyName} {ProductName}.  These containers will access data across your company's infrastructure and may store derived results in {CompanyName} {StorageName}.  As you define the workflow of your data analytics applications, you will need to access data across many different storage systems in your enterprise.  This access is beyond the scope of this architecture, but it is important to understand that data access will be required for a wide range of disparate storage systems.

The {CompanyName} {StorageName} cluster can be utilized to store both intermediary and final results from the data analytics pipeline.  In other words, as your data analytics applications derive new data results, those are typically stored in {CompanyName} {StorageName}.  This allows the data analytics environment ({ISVPartner} {ISVSolution}, {CompanyName} {ProductName}, and your data analytics applications) to be logically organized in one physical location.

=== Network
Networking is the technology component which typically requires the most advanced planning.  Your data analytics applications will have unique network access requirements, especially when integrated with existing IT infrastructure. For the physical level, using pairs of {IHVPartner} {IHVNetwork} Switch Series devices as top-of-rack (ToR) switches allows all servers, each having multiple 10GbE, or higher speed, NIC ports, to form a link aggregation group (LAG) across the ports with at least one NIC port on each switch in the stack. When using network interface bonding on the resource nodes, the LAG ideally offers switch redundancy within the rack and enables high availability throughout the infrastructure.

For this proof of concept deployment, a single 10GbE switch is used, having all resource nodes 10GbE NIC ports connected here, but could easily be expanded to the dual ToR configuration at a later date. To address management functions, the {IHVPlatformBMC}, out-of-band BMC interface can be connected to the same or another network switch infrastructure, but is fundamentally required to be accessible for this deployment.

The following considerations for the networking should be emphasized:

* Configure 802.3ad for system port bonding to get the maximum performance of bonded network interfaces
* Ensure that all similar switching devices are consistent and up-to-date with regard to firmware versions to reduce potential troubleshooting issues later.

== Summary

{IHVPartner} {IHVPlatform} is an excellent platform for creating a container runtime environment as is needed for {ISVPartner} {ISVSolution}.  Composible infrastructure allows you to define appropriate hardware from software descriptions.  This means you can easily scale, adjust and customize your environment to fit your needs, as you move from a proof-of-concept toward a production environment.

{CompanyName} {ProductName} is an enterprise Kubernetes container platform that provides software infrastructure for not only the {ISVPartner} {ISVSolution} software described in this reference, but also the data analytics applications you will build to ingest and manage your data.

All of the software environments in this reference architecture are supported products and have been tested to work together on {IHVPartner} {IHVPlatform} gear.


== Appendices / References

// NOTE: We intend to expand the list of reference links.

{ISVPartner} {ISVSolution}::
* https://www.sap.com/products/data-hub.html.
* Release Note - https://launchpad.support.sap.com/#/notes/2721708.
* Prerequisites - https://launchpad.support.sap.com/#/notes/2686169.
* Install Guide - https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.4.latest/en-US.

{CompanyName} {ProductName}::
* https://www.suse.com/products/caas-platform.
* Documentation - https://www.suse.com/documentation/suse-caasp-3.

{CompanyName} {StorageName}::
* https://www.suse.com/products/suse-enterprise-storage.
* Documentation - https://www.suse.com/documentation/suse-enterprise-storage-5.

=== {ISVPartner} Software Bill of Materials
include::{ISVPartner}-BOM.adoc[]

=== {CompanyName} Software Bill of Materials
include::{CompanyName}-BOM.adoc[]

=== {IHVPartner} Hardware Bill of Materials
include::{IHVPartner}-{IHVPlatform}-BOM.adoc[]
