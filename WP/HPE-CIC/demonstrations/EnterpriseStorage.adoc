:DemoTarget: EnterpriseStorage

include::../vars.adoc[]

ifdef::env-github[]
:imagesdir: {AuthorGHURL}/demonstrations/images/src/png/
endif::[]

= Demo : {CompanyName} {ProductNameStorage}
{Author}, {CompanyName} < {AuthorEMail} >

This document outlines the process to install {CompanyName} {ProductNameStorage} {ProductNameStorageVer} on a set of available virtual machines as a solution cluster in the {IHVPartner} {IHVPartnerEnv}. While citing steps, values or inputs which need to be changed from the default, documented process in this environment, you can refer to respective https://documentation.suse.com/ses/{ProductNameStorageVer}/single-html/ses-deployment/[{CompanyName} {ProductNameStorage} {ProductNameStorageVer} Deployment Guide] for more complete solution overview and details. 

// FixMe
TIP: Supplementary Videos - A condensed video version of this end-to-end process is available, within the {IHVPartner} {IHVPartnerEnv} at
video::https://www.youtube.com/watch?v=4VrhlyIgo3M[FixMe].  Likewise, there are full-length videos of the important process sections referenced throughout this document.

include::env-access.adoc[]

include::demo-infra.adoc[]

== Solution Cluster Deployment

Overview::
The following process encompasses an end-to-end deployment on Virtual Machines (VM), installation of the operating system and then creation of the cluster to provide the {CompanyName} {ProductNameStorage} solutin cluster. While many alternatives exist to create such a working solution cluster, the process outlined below is closely parallel to accomplishing the same thing on a set of baremetal nodes.

Important: Given the {IHVPartner} {IHVPartnerEnv} is potentially a shared demo environment, ensure these nodes are not in active use before recreating the solution from scratch!

include::vm-allocate.adoc[]

include::node-os.adoc[]

=== Deploying the Solution

After connecting via the VPN and logging into the jumphost, visit the vSphere Web Client, then follow the steps below:

TIP: For a recorded example video representative of this section's process, view video::https://drive.google.com/open?id=1AfpCwpbQnEL2IWlIGcKQW1ogqk7EnIHv[{CompanyName} {ProductNameStorage} Cluster Deployment].

. Right click on the respective virtual machine _{ProductNameStorage}-adm.{domainNameStorage}_ and _Open Remote Console_
. Login to the node as the "root" user
* Per the https://documentation.suse.com/ses/6/single-html/ses-deployment/#ceph-install-stack[Cluster Deployment] from the {CompanyName} {ProductNameStorage} Deployment Guide:
** Setup Salt configuration - master on "adm" node, minions on all solution cluster nodes
+
NOTE: Before accepting the minion keys, ensure the fully-qualified host names are represented by the values provided by each of the the salt-minions.
+
** On each of the "osd" nodes, zap all the non-operating system disk drives.
+
TIP: An example script to perform this action is available at: http://rmt.{domainName}/repo/EnterpriseStorage-deploy/[diskzap.sh].
+
** On the "adm" node, install "DeepSea"
** On each of the cluster nodes, create the Btrfs sub-volumes for /var/lib/ceph, since each of the "osd" nodes also have the "mon" functionality co-located on them.
** On the "adm" node, modify the _DriveGroups_ file to use all available drives on the "osd" nodes.
** Begin the cluster deployment, invoking the "preparation" and "discovery" stages:
+
----
deepsea stage run ceph.stage.0
deepsea stage run ceph.stage.1
----
** Given the rather minimal cluster node setup, adjust the following configurations:
*** Allow a cluster with only 3 "osd" nodes, by modifying a line on the "adm" node in the /srv/modules/runners/validate.py to:
+
----
if (not self.in_dev_env and len(storage) < 3) or (self.in_dev_env and len(storage) < 1)
----
+
*** Create a "/srv/pillar/ceph/proposals/policy.cfg" file to deploy the minimal functions.
+
TIP: An example configuration file is available at: http://rmt.{domainName}/repo/EnterpriseStorage-deploy/[policy.cfg].
+
** Finish the remaining stages of solution cluster deployment with:
+
[subs="attributes"]
----
deepsea stage run ceph.stage.2
deepsea stage run ceph.stage.3
deepsea stage run ceph.stage.4
----
*** Then verify the cluster, via
+
----
ceph status
----

== Summary
At this point, the {CompanyName} {ProductNameStorage} solution cluster should be up, running and functional.  

//FixMe
TBD - Only included base solution deployment so far, so may add additional sections over time.
NOTE: In the interim, refer to https://documentation.suse.com/ses/{ProductNameStorageVer}/single-html/ses-admin[{CompanyName} {ProductNameStorage} Administration Guide] for operational aspects and guidance.

== Appendices
TBD - Will link to other content over time
//FixMe
// * RMT Setup
// * DNS Setup
// * LB Setup
